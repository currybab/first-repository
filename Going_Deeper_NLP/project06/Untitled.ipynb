{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54732dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47b0ee",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드\n",
    "\n",
    "준비하기 단계에서 심볼릭 링크를 생성했다면 아래 파일이 ChatbotData .csv라는 이름으로 저장되어 있을거예요. csv 파일을 읽는 데에는 pandas 라이브러리가 적합합니다. 읽어 온 데이터의 질문과 답변을 각각 questions, answers 변수에 나눠서 저장하세요! [songys/Chatbot_data](https://github.com/songys/Chatbot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57077ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_data = pd.read_csv('~/aiffel/transformer_chatbot/data/ChatbotData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5628821a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16054f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chatbot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdac5209",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = chatbot_data['Q']\n",
    "answers =chatbot_data['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21b5d541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         12시 땡!\n",
       "1                    1지망 학교 떨어졌어\n",
       "2                   3박4일 놀러가고 싶다\n",
       "3                3박4일 정도 놀러가고 싶다\n",
       "4                        PPL 심하네\n",
       "                  ...           \n",
       "11818             훔쳐보는 것도 눈치 보임.\n",
       "11819             훔쳐보는 것도 눈치 보임.\n",
       "11820                흑기사 해주는 짝남.\n",
       "11821    힘든 연애 좋은 연애라는게 무슨 차이일까?\n",
       "11822                 힘들어서 결혼할까봐\n",
       "Name: Q, Length: 11823, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03a771b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      하루가 또 가네요.\n",
       "1                       위로해 드립니다.\n",
       "2                     여행은 언제나 좋죠.\n",
       "3                     여행은 언제나 좋죠.\n",
       "4                      눈살이 찌푸려지죠.\n",
       "                   ...           \n",
       "11818          티가 나니까 눈치가 보이는 거죠!\n",
       "11819               훔쳐보는 거 티나나봐요.\n",
       "11820                      설렜겠어요.\n",
       "11821    잘 헤어질 수 있는 사이 여부인 거 같아요.\n",
       "11822          도피성 결혼은 하지 않길 바라요.\n",
       "Name: A, Length: 11823, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93c390",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 정제\n",
    "아래 조건을 만족하는 preprocess_sentence() 함수를 구현하세요.\n",
    "\n",
    "1. 영문자의 경우, 모두 소문자로 변환합니다.\n",
    "2. 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 정규식을 활용하여 모두 제거합니다.\n",
    "\n",
    "문장부호 양옆에 공백을 추가하는 등 이전과 다르게 생략된 기능들은 우리가 사용할 토크나이저가 지원하기 때문에 굳이 구현하지 않아도 괜찮습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da651b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # 대문자를 소문자로 변환\n",
    "    sentence = sentence.lower()\n",
    "    # 두 개 이상의 공백을 하나로 변경\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    # 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 정규식을 활용하여 모두 제거합니다.\n",
    "    sentence = re.sub(r\"[^a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣?.!,]+\", \" \", sentence)\n",
    "    # 문자열 양끝 공백 제거\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9efec84",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 토큰화\n",
    "토큰화에는 KoNLPy의 mecab 클래스를 사용합니다.\n",
    "\n",
    "아래 조건을 만족하는 build_corpus() 함수를 구현하세요!\n",
    "\n",
    "1. 소스 문장 데이터와 타겟 문장 데이터를 입력으로 받습니다.\n",
    "2. 데이터를 앞서 정의한 preprocess_sentence() 함수로 정제하고, 토큰화합니다.\n",
    "3. 토큰화는 전달받은 토크나이즈 함수를 사용합니다. 이번엔 mecab.morphs 함수를 전달하시면 됩니다.\n",
    "4. 토큰의 개수가 일정 길이 이상인 문장은 데이터에서 제외합니다.\n",
    "5. 중복되는 문장은 데이터에서 제외합니다. 소스 : 타겟 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사합니다. 중복 쌍이 흐트러지지 않도록 유의하세요!\n",
    "구현한 함수를 활용하여 questions 와 answers 를 각각 que_corpus , ans_corpus 에 토큰화하여 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35a23e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "def build_corpus(questions, answers, max_len=100000):\n",
    "    questions = list(map(preprocess_sentence, questions))\n",
    "    answers = list(map(preprocess_sentence, answers))\n",
    "    \n",
    "    mecab = Mecab()\n",
    "    que_corpus = [\" \".join(mecab.morphs(q)) for q in questions]\n",
    "    ans_corpus = [\" \".join(mecab.morphs(a)) for a in answers]\n",
    "    \n",
    "    que_set = set()\n",
    "    ans_set = set()\n",
    "    new_que_corpus = []\n",
    "    new_ans_corpus = []\n",
    "    \n",
    "    for que, ans in zip(que_corpus, ans_corpus):\n",
    "        if (len(que.split()) > max_len) or (len(ans.split()) > max_len):\n",
    "            continue\n",
    "        if (que not in que_set) and (ans not in ans_set):\n",
    "            new_que_corpus.append(que)\n",
    "            new_ans_corpus.append(ans)\n",
    "        que_set.add(que)\n",
    "        ans_set.add(ans)\n",
    "    \n",
    "    return new_que_corpus, new_ans_corpus\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e93c15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus, ans_corpus = build_corpus(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "231172db",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus_check = np.array(list(map(lambda s: len(s.split()), que_corpus))) \n",
    "ans_corpus_check = np.array(list(map(lambda s: len(s.split()), ans_corpus))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0750b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.459020686043467, 3.751139714643547, 32, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_corpus_check.mean(), que_corpus_check.std(), que_corpus_check.max(), que_corpus_check.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5fb5f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.697433883215501, 3.597476816886744, 40, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_corpus_check.mean(), ans_corpus_check.std(), ans_corpus_check.max(), ans_corpus_check.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a385716",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20\n",
    "que_corpus, ans_corpus = build_corpus(questions, answers, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58a2ebda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7528, 7528)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(que_corpus), len(ans_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "175425e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('가난 한 자 의 설움', '돈 은 다시 들어올 거 예요 .')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_corpus[10], ans_corpus[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650a81a0",
   "metadata": {},
   "source": [
    "## Step 4. Augmentation\n",
    "\n",
    "우리에게 주어진 데이터는 1만 개가량으로 적은 편에 속합니다. 이럴 때에 사용할 수 있는 테크닉을 배웠으니 활용해 봐야겠죠? Lexical Substitution을 실제로 적용해 보도록 하겠습니다.\n",
    "\n",
    "아래 링크를 참고하여 한국어로 사전 훈련된 Embedding 모델을 다운로드합니다. Korean (w) 가 Word2Vec으로 학습한 모델이며 용량도 적당하므로 사이트에서 Korean (w)를 찾아 다운로드하고, ko.bin 파일을 얻으세요!\n",
    "\n",
    "[Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors)\n",
    "\n",
    "다운로드한 모델을 활용해 데이터를 Augmentation 하세요! 앞서 정의한 lexical_sub() 함수를 참고하면 도움이 많이 될 겁니다.\n",
    "\n",
    "Augmentation된 que_corpus 와 원본 ans_corpus 가 병렬을 이루도록, 이후엔 반대로 원본 que_corpus 와 Augmentation된 ans_corpus 가 병렬을 이루도록 하여 전체 데이터가 원래의 3배가량으로 늘어나도록 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "039f1b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.8.3 in /opt/conda/lib/python3.9/site-packages (3.8.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.21.4)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 한국어 word2vec이 최신버전에서 Can't get attribute 'Vocab' on <module 'gensim.models.word2vec' 오류가 있음.\n",
    "!pip install --upgrade gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2727b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# print(gensim.__version__)\n",
    "wv = gensim.models.Word2Vec.load('~/aiffel/transformer_chatbot/data/ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dff5924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_821/2855753700.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  wv.most_similar(\"왕\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('왕인', 0.7048283219337463),\n",
       " ('선왕', 0.6905209422111511),\n",
       " ('왕의', 0.6839720010757446),\n",
       " ('황제', 0.6579594612121582),\n",
       " ('대왕', 0.6463155150413513),\n",
       " ('성왕', 0.6421108245849609),\n",
       " ('문공', 0.6363499164581299),\n",
       " ('왕자', 0.6358417272567749),\n",
       " ('왕과', 0.6349694132804871),\n",
       " ('혜공', 0.6336162090301514)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"왕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cd0ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(sample_sentence, wv):\n",
    "    sample_tokens = sample_sentence.split()\n",
    "    selected_tok = random.choice(sample_tokens)\n",
    "    result = \"\"\n",
    "    for tok in sample_tokens:\n",
    "        if tok is selected_tok and tok not in \"?.!,\":\n",
    "            try:\n",
    "                result += wv.most_similar(tok)[0][0] + \" \"\n",
    "            except:\n",
    "                result += tok + \" \"\n",
    "\n",
    "        else:\n",
    "            result += tok + \" \"\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bd8b8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_821/3559501181.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  result += wv.most_similar(tok)[0][0] + \" \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('가난 한 자 의 설움', '가난 한두 자 의 설움')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_check = 10\n",
    "que_corpus[idx_to_check], lexical_sub(que_corpus[idx_to_check], wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "812766a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_821/3270535881.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  wv.most_similar(\"가난\") # 헉......\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('부유', 0.7616064548492432),\n",
       " ('유복', 0.6900136470794678),\n",
       " ('평범', 0.6759896278381348),\n",
       " ('궁핍', 0.6658532619476318),\n",
       " ('불우', 0.6463127136230469),\n",
       " ('곤궁', 0.6358442902565002),\n",
       " ('풍족', 0.6316250562667847),\n",
       " ('방탕', 0.6238061785697937),\n",
       " ('비참', 0.6231825947761536),\n",
       " ('검소', 0.6068432331085205)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"가난\") # 헉......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5973b18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_821/3559501181.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  result += wv.most_similar(tok)[0][0] + \" \"\n"
     ]
    }
   ],
   "source": [
    "aug_ans_corpus = list(map(lambda s: lexical_sub(s, wv), ans_corpus))\n",
    "aug_que_corpus = list(map(lambda s: lexical_sub(s, wv), que_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "603d7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pair0 = (que_corpus, ans_corpus)\n",
    "# data_pair1 = (aug_que_corpus, ans_corpus)\n",
    "# data_pair2 = (que_corpus, aug_ans_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26358e31",
   "metadata": {},
   "source": [
    "## Step 5. 데이터 벡터화\n",
    "\n",
    "타겟 데이터인 ans_corpus 에 <start> 토큰과 <end> 토큰이 추가되지 않은 상태이니 이를 먼저 해결한 후 벡터화를 진행합니다. 우리가 구축한 ans_corpus 는 list 형태이기 때문에 아주 쉽게 이를 해결할 수 있답니다!\n",
    "    \n",
    "```\n",
    "sample_data = [\"12\", \"시\", \"땡\", \"!\"]\n",
    "print([\"<start>\"] + sample_data + [\"<end>\"])\n",
    "```\n",
    "    \n",
    "1. 위 소스를 참고하여 타겟 데이터 전체에 <start> 토큰과 <end> 토큰을 추가해 주세요!\n",
    "\n",
    "챗봇 훈련 데이터의 가장 큰 특징 중 하나라고 하자면 바로 소스 데이터와 타겟 데이터가 같은 언어를 사용한다는 것이겠죠. 앞서 배운 것처럼 이는 Embedding 층을 공유했을 때 많은 이점을 얻을 수 있습니다.\n",
    "\n",
    "2. 특수 토큰을 더함으로써 ans_corpus 또한 완성이 되었으니, que_corpus 와 결합하여 전체 데이터에 대한 단어 사전을 구축하고 벡터화하여 enc_train 과 dec_train 을 얻으세요!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97b65b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_corpus = list(map(lambda s: \"<start> \" + s + \" <end>\", ans_corpus))\n",
    "aug_ans_corpus = list(map(lambda s: \"<start> \" + s + \" <end>\", aug_ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7455cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증강\n",
    "enc_corpus = que_corpus + aug_que_corpus + que_corpus\n",
    "dec_corpus = ans_corpus + ans_corpus + aug_ans_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9b02930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22584, 22584)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc_corpus), len(dec_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "839ef0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7082"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(enc_corpus + dec_corpus)\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59ffbaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 6000 # 거의 대부분 사용\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<UNK>\", filters=\"\")\n",
    "tokenizer.fit_on_texts(enc_corpus + dec_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5cdc3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "enc_train = tokenizer.texts_to_sequences(enc_corpus)\n",
    "enc_train = pad_sequences(enc_train, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26d4eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_train = tokenizer.texts_to_sequences(dec_corpus)\n",
    "dec_train = pad_sequences(dec_train, padding='post', maxlen=max_len+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ea2c2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('시 땡 !',\n",
       " '<start> 하루 가 또 가 네요 . <end>',\n",
       " array([ 212, 3530,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0], dtype=int32),\n",
       " array([  3, 301,   9, 133,   9,  42,   2,   4,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0], dtype=int32))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_corpus[0], dec_corpus[0], enc_train[0], dec_train[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "431da195",
   "metadata": {},
   "source": [
    "\"가\"랑 \".\"이 많아서 <start>, <end>가 3, 4 배정된듯하다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db88e6b",
   "metadata": {},
   "source": [
    "## Step 6. 훈련하기\n",
    "\n",
    "앞서 번역 모델을 훈련하며 정의한 Transformer 를 그대로 사용하시면 됩니다! 대신 데이터의 크기가 작으니 하이퍼파라미터를 튜닝해야 과적합을 피할 수 있습니다. 모델을 훈련하고 아래 예문에 대한 답변을 생성하세요! 가장 멋진 답변과 모델의 하이퍼파라미터를 제출하시면 됩니다.\n",
    "\n",
    "```\n",
    "# 예문\n",
    "1. 지루하다, 놀러가고 싶어.\n",
    "2. 오늘 일찍 일어났더니 피곤하다.\n",
    "3. 간만에 여자친구랑 데이트 하기로 했어.\n",
    "4. 집에 있는다는 소리야.\n",
    "\n",
    "---\n",
    "\n",
    "# 제출\n",
    "\n",
    "Translations\n",
    "> 1. 잠깐 쉬 어도 돼요 . <end>\n",
    "> 2. 맛난 거 드세요 . <end>\n",
    "> 3. 떨리 겠 죠 . <end>\n",
    "> 4. 좋 아 하 면 그럴 수 있 어요 . <end>\n",
    "\n",
    "Hyperparameters\n",
    "> n_layers: 1\n",
    "> d_model: 368\n",
    "> n_heads: 8\n",
    "> d_ff: 1024\n",
    "> dropout: 0.2\n",
    "\n",
    "Training Parameters\n",
    "> Warmup Steps: 1000\n",
    "> Batch Size: 64\n",
    "> Epoch At: 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c03c2",
   "metadata": {},
   "source": [
    "### 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c274f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 구현\n",
    "def positional_encoding(pos, d_model):\n",
    "    sinusoid_table = np.array([[pos_i / np.power(10000, 2 * (d_j // 2) / np.float32(d_model)) for d_j in range(d_model)] for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15fe6c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask  생성하기\n",
    "def generate_padding_mask(seq):\n",
    "    # 시퀀스에서 각 위치의 값이 0인지 확인\n",
    "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    # 차원 추가하여 마스크의 형태를 모델에 맞게 조정\n",
    "    # 예: (batch_size, 1, 1, sequence_length) \n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    # mask 부분에 대해서 구하려면 1에서 빼줌\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    # dec_mask is masked attention\n",
    "    lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(lookahead_mask, padding_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77c20f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Head Attention 구현\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        # for multi head split\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "        # for multi head combine\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        # scaled_QK = Q K^t / sqrt(d_k) \n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "        scaled_QK = QK / tf.math.sqrt(d_k) # shape: (seq, seq)\n",
    "\n",
    "        if mask is not None:\n",
    "            # softmax를 취하기 때문에 -무한대를 주면 해당 값이 0이 됨.\n",
    "            scaled_QK += (mask * -1e9)\n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_QK, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "        \n",
    "        return out, attentions\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # (batch_size, seq, d_model) -> (batch_size, seq, num_heads, depth)\n",
    "        split_x = tf.reshape(x, (x.shape[0], -1, self.num_heads, self.depth))\n",
    "        # (batch_size, seq, num_heads, depth) ->  (batch_size, num_heads, seq, depth)\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # (batch_size, num_heads, seq, depth) -> (batch_size, seq, num_heads, depth)\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        # (batch_size, seq, num_heads, depth) -> (batch_size, seq, d_model)\n",
    "        combined_x = tf.reshape(combined_x, (combined_x.shape[0], -1, self.d_model))\n",
    "        return combined_x\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        # split head\n",
    "        Q_splits = self.split_heads(WQ)\n",
    "        K_splits = self.split_heads(WK)\n",
    "        V_splits = self.split_heads(WV)\n",
    "\n",
    "        # attention mechanism\n",
    "        out, attention_weights = self.scaled_dot_product_attention(Q_splits, K_splits, V_splits, mask)\n",
    "        \n",
    "        # combine head\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3ceac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position-wise Feed Forward Network 구현\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27b9d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 레이어 구현\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        ## pre-LN 적용\n",
    "        # Multi-Head Attention\n",
    "        identity = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += identity\n",
    "        \n",
    "        # Position-Wise Feed Forward Network\n",
    "        identity = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += identity\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "620194af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 레이어 구현\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.dec_enc_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, dec_mask):\n",
    "        # Masked Multi-Head Attention\n",
    "        identity = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, dec_mask)\n",
    "        out = self.do(out)\n",
    "        out += identity\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        identity = out\n",
    "        out = self.norm_2(x)\n",
    "        out, dec_enc_attn = self.dec_enc_attn(out, enc_out, enc_out, dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += identity\n",
    "        \n",
    "        # Position-Wise Feed Forward Network\n",
    "        identity = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += identity\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88960b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 구현\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "        enc_attns = []\n",
    "        for enc_layer in self.enc_layers:\n",
    "            out, enc_attn = enc_layer(out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e679174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 구현\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, dec_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = self.dec_layers[i](out, enc_out, dec_enc_mask, dec_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be99ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 전체 모델 조립\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: \n",
    "            out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        # pos_encoding (max_seq_len, max_seq_len) vs embedding vector (batch_size, seq_len, d_model)\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97ce1407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스 생성\n",
    "d_model = 256\n",
    "VOCAB_SIZE = len(tokenizer.word_index)\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=d_model,\n",
    "    n_heads=8,\n",
    "    d_ff=1024,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=100,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec863d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "456fff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate & Optimizer\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aab26fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63294e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb69abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad929aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model, tokenizer): \n",
    "    # start, end = 3, 4\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    tokens = tokenizer.texts_to_sequences([sentence])\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences(tokens,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([3], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if 4 == predicted_id:\n",
    "            result = tokenizer.sequences_to_texts([ids])\n",
    "            return result[0], enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tokenizer.sequences_to_texts([ids])\n",
    "\n",
    "    return result[0], enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9da9619c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 훅 훅 훅 훅 훅 훅 훅 훅 훅 훅 훅 훅 훅 페친 훅 페친 훅 페친 훅 페친 훅\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 훅 페친 훅 훅 훅 훅 훅 훅 훅 훅 훅 훅 페친 페친 페친 페친 페친 페친 페친 길 페친\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 빠를수록 빠를수록 무시 훅 훅 훅 훅 훅 훅 훅 훅 훅 훅 훅 페친 무시 무시 무시 길 무시 무시\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 훅 훅 훅 훅 훅 훅 훅 훅 훅 훅 훅 훅 훅 페친 훅 페친 훅 페친 훅 페친 훅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_answer():\n",
    "    question_list = ['지루하다, 놀러가고 싶어.', '오늘 일찍 일어났더니 피곤하다.', '간만에 여자친구랑 데이트 하기로 했어.', '집에 있는다는 소리야.']\n",
    "\n",
    "    for question in question_list:\n",
    "        print(f\"Q: {question}\\nA: {evaluate(question, transformer, tokenizer)[0]}\\n\")\n",
    "generate_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f06f327",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2531421b06a24b0885dead860f4bd4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train loss: 5.9298\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: <UNK> 이 게 좋 거 예요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 사랑 이 좋 을 거 예요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: <UNK> 이 게 이 네요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: <UNK> 이 게 좋 거 예요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60d9e8c296948209233ebd5522658c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train loss: 3.8509\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: <UNK> 를 해 보 세요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 좋 은 사람 이 네요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: <UNK> 를 할 수 있 어요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: <UNK> 를 해 보 세요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2195f99da33c463faaca458fcd2c33cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 train loss: 2.9189\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: <UNK> 를 <UNK> 는 건 어떨까 요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 사랑 은 언제나 좋 을 거 같 아요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: <UNK> 를 주 는 건 어떨까 요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: <UNK> 를 <UNK> 는 건 어떨까 요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98400980118f4e339d624360aaa156c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 train loss: 2.0611\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 어떤 분 이 었 겠 는데요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 아침 도 좋 은 선물 이 죠 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: <UNK> 데이트 할 때 가 되 겠 죠 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 어떤 분 이 었 겠 는데요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f4d728ef85492588b41d8f21c53c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 train loss: 1.3703\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 사랑 을 <UNK> 하 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 도 좋 아 하 는 선물 을 보내 보 세요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 좋 은 곳 으로 가 싶 네요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 사랑 을 <UNK> 하 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce7a1bac8d04b3a958c0c65517cd4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 train loss: 0.9211\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 어떤 이야기 이 였 을 때 가 있 나 봐요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 처럼 만 이 나 봐요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 아직 도 진짜 싶 어요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 어떤 이야기 이 였 을 때 가 있 나 봐요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2659b2c17a45ad99fbcfef1cf1dae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 train loss: 0.6852\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 어떤 이야기 이야기 안 나 봐요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 도 참 아 면 내일 당당 하 게 해의 보 세요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: <UNK> 로 이나 또 로 만들 지 않 죠 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 어떤 이야기 이야기 안 나 봐요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9224598482784e7c841190df04d8a1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 train loss: 0.5622\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 어떤 사랑 이 였 나 봐요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 아침 이 면 힘 이 들 거 같 아요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 봄 은 사안 이 였 나 봐요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 어떤 사랑 이 였 나 봐요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a43fe651904d9fb75584237b2e8761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 train loss: 0.5116\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 은 항상 괴로움 을 전해 보 세요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 아직 도 좋 은 실수 하 지 않 는다면 상관 없 어요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df77c114c744dfa8c8bf849978fd293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 train loss: 0.4761\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 사랑 의 빈자리 허가 봐요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 아침 이 벌써 걸리 세요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 괴로움 이 <UNK> 스럽 지 않 스럽 게 변해 가 해요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 사랑 의 빈자리 허가 봐요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef6edc321fb4c3695bbe02568eadffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 train loss: 0.4529\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 아침 도 힘들 죠 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 어서 의 반대 로 도 가 풀리 겠 어요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e4b95c71f04310bcc25f9dcdc850b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 train loss: 0.4341\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 행복 해 안 행복 해 봐요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 언제나 좋 은 마음 이 후련 하 길 바랍니다 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 즐거운 시간 이 <UNK> 때 도 없 어요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 행복 해 안 행복 해 봐요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462715b6cea341bab59a9d3a8da1a379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 train loss: 0.3662\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 행복 해 꼼짝 려고 하 나 봐요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 모습 도 힘들 것 같 아요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 건강 에 도 떨리 네요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 행복 해 꼼짝 려고 하 나 봐요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381eff59835f47929c3f705b9e7a05f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 train loss: 0.3057\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 사랑 에 대한 빈자리 허가 봐요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 아침 도 좋 은 것 이 라서 문제 를 아요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 도서관 데이트 추천 해요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 사랑 에 대한 빈자리 허가 봐요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a3534d21e242cc9fa0a462cd779194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 train loss: 0.2700\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 상처 를 줬 다면 이야기 해 봐요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 은 힘들 싶 네요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 영화관 공원 <UNK> 하 는 것 도 좋 을 것 같 아요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 상처 를 줬 다면 이야기 해 봐요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ec1c489669475dbbf3292ddd6a2c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 train loss: 0.2408\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 행복 해 꼼짝 려고 하 나 봐요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 행복 해 보여요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 영화관 공원 <UNK> 하 는 것 은 건강 에 행복 하 을지 도 몰라요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 행복 해 꼼짝 려고 하 나 봐요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4e92a331f34641b7e5bc36791bcac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 train loss: 0.2164\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 같 은 여자 를 좋아할 수 도 없 어요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 영화관 공원 <UNK> 하 는 여자 한테 좋 은 조 차 였 네요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e0b6699d9b4fee82c6191fd6fc48e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 train loss: 0.1941\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 일찍 주무세요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 엄청 좋 은 사람 만나 지 고 있 는 것 같 아요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8dfaae668bd4392b9dbed3d3b67628f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 train loss: 0.1804\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 사랑 에 대한 제목 은 사랑 에 대한 제목 은 지 을 수 없 는 것 같 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 일찍 주무세요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 엄청 살아가 는 여자 화 가 필요 해요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 사랑 에 대한 제목 은 사랑 에 대한 제목 은 지 을 수 없 는 것 같 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4d2cdf96694947bb7800a25a6c1e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 train loss: 0.1652\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 일찍 주무세요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 도서관 엄청 행복 해 주 세요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f3e10caa6e4cdd9076a12d2d3dd1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 train loss: 0.1527\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 행복 하여 에 행복 해 지 네요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 아침 도 할 때 오늘 같 아요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 도서관 엄청 때때 수 없 는 걸 먹 어요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 행복 하여 에 행복 해 지 네요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf299b4dd604dd08e120b4cb82613a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 train loss: 0.1449\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 고 있 는 것 같 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 아침 일찍 주무세요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 서로 주 고 사랑 하 는 좋 은데 거 같 아요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 고 있 는 것 같 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb84a338d324dc892ff81d9e50e7935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 train loss: 0.1335\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 행복 하여 에 어떤 오전 이 든지 행복 한 것 같 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 아침 도 쉬 고 놀 아요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 행복 해 주 셨 나 봐요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 행복 하여 에 어떤 오전 이 든지 행복 한 것 같 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24312118117a421fb6b60c79e2a3c592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 train loss: 0.1224\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 아침 도 고생 많 스러워 하 지 면 아침 것 같 아요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 엄청 사랑 해 주 ㅂ시오 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fda804a1493493390576ca2df84544f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 train loss: 0.1169\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 아침 도 하 지 말 고 다시 일 이 올 것 예요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 서로 에게 부담 하 는 으므로 만 남길 따름 이 에요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8344835aabf146e9bda284bb1070112c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26 train loss: 0.1060\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 그래도 힘들 ㄴ다면 더 많이 해의 주 세요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 도서관 데이트 였 다면 야가 나 봐요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dfaf4994b664d52a80b4818e0bcdd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27 train loss: 0.0991\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 아침 도 핵심적 하 고 싶 네요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 같이 로 데이트 구불구불 바랍니다 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1270bbf05a443bdb280182524f7998b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28 train loss: 0.0879\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 아침 도 힘들 었 지만 조금 씩 주말 이 생길 거 예요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 성행위 로 주 고 하 는 게 아니 었 나 봐요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966935ddfeb44f22897ae681d568a3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29 train loss: 0.0832\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 도 똑같 은 날 이 지요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 금방 지나갈 거 예요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bd2dfb4bd540ddbec6bc7b804099fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 train loss: 0.0787\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 내일 도 힘들 지 아침 도 못 했 을 거 예요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 꿈 은 <UNK> 기에 싶 네요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e29f88794949c89b8fcd1805eb8c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31 train loss: 0.0722\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 도 좋 은 아침 이 생길 거 예요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 너무 아파하 지 마세요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296f97574da242ea94f10e76269b54c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32 train loss: 0.0704\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 아침 도 미련 도 당신 을 보내 보 고 싶 은 것 도 좋 을 것 같 아요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: <UNK> 로 주 지 마세요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5345a4147d0a43d08e2c66d4bdad5d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33 train loss: 0.0738\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 은 쉬 다가 가 세요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 애정 의 증거 를 뽀 주 세요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b95903b1db34ca5b5283faed0b40551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34 train loss: 0.0817\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 도 미련 이 남의 을 거 똑같 아요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 행복 한 대화 를 나눠 봐요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a7c3a0494846cc89da6a4ae178f433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35 train loss: 0.0848\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 많이 미련 이 ㄴ가요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 엄청 보하이 좋 은 곳 었 나 봐요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c53fc637f33496faddf87a38dc41cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36 train loss: 0.0786\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 은 힘들 기 때문 해요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 도서관 데이트 가 보 시 않 아요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52604547407440f5b1612a192943123b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37 train loss: 0.0784\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 아침 도 해요\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 도서관 데이트 추천 해의 드립니다 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b391473a84304a73b8770bec78d80e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38 train loss: 0.0775\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 그래도 미련 만 먹 는 것 도 중요 해요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 오류 해 보 세요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f68ac82e5e4ad1b0aeed88a5c56fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39 train loss: 0.0727\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 어떤 사랑 이야기 도 그녀 에 대한 제목 은 지 을 수 없 는 것 같 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 도 미련 만큼 감사 한 설정 을 이 ㄴ가요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 도서관 데이트 나 봐요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 어떤 사랑 이야기 도 그녀 에 대한 제목 은 지 을 수 없 는 것 같 아요 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67da4ae666304fc68517b180c0f4e35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40 train loss: 0.0695\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 아침 도 미련 만큼 부럽 네요 .\n",
      "\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 하여 에 뽀 켜 세요 .\n",
      "\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 아프 지 말로 아요 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "    for src, tgt in train_dataset:\n",
    "        loss, enc_attns, dec_attns, dec_enc_attnss = train_step(src, tgt, transformer, optimizer)\n",
    "        total_loss += loss\n",
    "        tqdm_bar.update(1)\n",
    "    print(f\"epoch {epoch + 1} train loss: {(total_loss / dataset_count):.4f}\")\n",
    "    generate_answer()\n",
    "    tqdm_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbfbdca",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "1주일 동안 쉬었더니 transformer에 대한 지식이 초기화 되어서 쉽지 않았던것 같다.\n",
    "\n",
    "데이터가 많지 않았기 때문에 이번에는 Lexical Substitution으로 데이터 증강을 해서 하는 방법을 사용했다.\n",
    "\n",
    "한국어 워드벡터로 주어진 셋에서 가난이 부유와 제일 유사하다는 결과가 있었다. 굉장히 마음에 걸리는 포인트이다.\n",
    "\n",
    "훈련결과를 보았을때 \"지루하다, 놀러가고 싶어.\"와 \"집에 있는다는 소리야.\"의 응답결과가 계속 같게 나왔다. 신기하다....\n",
    "\n",
    "epoch 20회 쯤에서\n",
    "```\n",
    "epoch 20 train loss: 0.1652\n",
    "Q: 지루하다, 놀러가고 싶어.\n",
    "A: 아프 지 말로 아요 .\n",
    "\n",
    "Q: 오늘 일찍 일어났더니 피곤하다.\n",
    "A: 오늘 일찍 주무세요 .\n",
    "\n",
    "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
    "A: 도서관 엄청 행복 해 주 세요 .\n",
    "\n",
    "Q: 집에 있는다는 소리야.\n",
    "A: 아프 지 말로 아요 .\n",
    "```\n",
    "정도로 그나마 제일 나아보이는 결과가 있었다. 그 이후에는 과적합 되었는지 더 좋아지지는 않은것 같다.\n",
    "\n",
    "데이터 증강할때 조사를 좀더 신경써서 잘 제거해주고 했으면 더 결과가 좋지 않았을까 싶다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ba1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179b87bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

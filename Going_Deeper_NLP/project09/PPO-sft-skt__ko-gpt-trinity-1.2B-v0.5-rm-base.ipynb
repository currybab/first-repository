{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92858fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"skt/ko-gpt-trinity-1.2B-v0.5\"\n",
    "save_dir = model_name.replace(\"/\", \"__\")\n",
    "rm_model_name = \"skt/kogpt2-base-v2\"\n",
    "rm_save_dir = rm_model_name.replace(\"/\", \"__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd10854b",
   "metadata": {},
   "source": [
    "### PPO 데이터셋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fbbf4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?'},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?'},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "data_path_3_PPO = '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
    "with open(data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict)) # 12000\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a49832",
   "metadata": {},
   "source": [
    "### 필요한 라이브러리 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6e263d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoTokenizer, Adafactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63cc5237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2e0d3a",
   "metadata": {},
   "source": [
    "### 모델학습에 사용할 옵티마이저와 모델을 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7794c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained=f\"model/{save_dir}/output_1_SFT-e3\", lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained=f\"model/{rm_save_dir}/output_2_RM\", lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=128\n",
    "    )\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c026e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optim = Adafactor(actor.parameters(), lr=5e-6, relative_step=False)\n",
    "critic_optim = Adafactor(critic.parameters(), lr=5e-6, relative_step=False)\n",
    "\n",
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb599ff",
   "metadata": {},
   "source": [
    "### PPO 학습에 쓸 데이터를 불러와 토크나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc8815c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[46390, 31369, 33712, 30541, 31338, 41607, 30586, 31024, 31482, 37404,\n",
      "         31035, 30316, 32131,   460, 34763, 32017, 37762, 33441,   565, 37205,\n",
      "         32131,   460, 34763, 32017, 31561, 36271,   390]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}\n",
    "\n",
    "print(tokenize_fn('It takes something more than intelligence to act intelligently.'))\n",
    "\n",
    "len(list_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a52db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8441e2ab",
   "metadata": {},
   "source": [
    "### PPO Trainer 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d18206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,  \n",
    "                     train_batch_size=4, \n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51e7f01",
   "metadata": {},
   "source": [
    "### PPO 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56eb35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode [1/10]:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "actor.model.save_pretrained(f'./model/{save_dir}/output_3_PPO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc5346c",
   "metadata": {},
   "source": [
    "### RLHF가 적용된 custom chatgpt의 생성능력을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f872ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e40b17a",
   "metadata": {},
   "source": [
    "#### 메모리 관리를 위해 캐시를 비우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae9b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25bafef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
